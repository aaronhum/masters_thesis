{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrowing down to probes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.impute import SimpleImputer  # For imputation in Method 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrowing down to probes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup env for multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcawg_prim_window_meth_df_merged = pd.read_csv(\"../_OUTPUTS_/merged_sith_meth_pcawg_prim_window.tsv\", sep = \"\\t\")\n",
    "pcawg_sith_corr_meth_df_merged = pd.read_csv(\"../_OUTPUTS_/merged_sith_meth_pcawg_sith_corr.tsv\", sep = \"\\t\")\n",
    "pcawg_iqr_corr_meth_df_merged = pd.read_csv(\"../_OUTPUTS_/merged_sith_meth_pcawg_iqr_corr.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to '../_OUTPUTS_/pcawg_probes_of_interest.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define bins and labels for the sith_level column\n",
    "bins = [-float('inf'), 0.7, 0.8, float('inf')]\n",
    "labels = ['<0.7', '0.7-0.8', '>0.8']\n",
    "\n",
    "# Create a list of datasets and key parameters.\n",
    "# Note: For the IQR dataset we sort on the 'INT_IQR' column, but for the others we sort on 'SITH'.\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'pcawg_prim_window',\n",
    "        'df': pcawg_prim_window_meth_df_merged,\n",
    "        'sort_col': 'SITH'\n",
    "    },\n",
    "    {\n",
    "        'name': 'pcawg_sith_corr',\n",
    "        'df': pcawg_sith_corr_meth_df_merged,\n",
    "        'sort_col': 'SITH'\n",
    "    },\n",
    "    {\n",
    "        'name': 'pcawg_iqr_corr',\n",
    "        'df': pcawg_iqr_corr_meth_df_merged,\n",
    "        'sort_col': 'INT_IQR'\n",
    "    }\n",
    "]\n",
    "\n",
    "# A list to gather all result rows\n",
    "results_list = []\n",
    "\n",
    "# Loop over each dataset and perform the pipeline of analyses.\n",
    "for dset in datasets:\n",
    "    name = dset['name']\n",
    "    df = dset['df']\n",
    "    sort_col = dset['sort_col']\n",
    "    \n",
    "    # --- Step 1: Create the 'sith_level' column ---\n",
    "    df['sith_level'] = pd.cut(df['SITH'], bins=bins, labels=labels)\n",
    "\n",
    "    # --- Step 2: Sort the DataFrame ---\n",
    "    sorted_df = df.sort_values(sort_col, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # --- Step 3: Extract the probe columns (columns starting with 'cg') ---\n",
    "    probe_cols = [col for col in sorted_df.columns if col.startswith('cg')]\n",
    "    probe_cols = list(probe_cols)\n",
    "    data_matrix = sorted_df[probe_cols]\n",
    "\n",
    "    # --- Step 4: Filter probes based on initial conditions ---\n",
    "    # Remove probes where all values are ≤ 0.5 or all values are ≥ 0.5\n",
    "    mask = (data_matrix > 0.5).any(axis=0) & (data_matrix < 0.5).any(axis=0)\n",
    "    probe_cols = data_matrix.columns[mask]\n",
    "    probe_cols = list(probe_cols)\n",
    "    data_matrix = data_matrix[probe_cols]\n",
    "\n",
    "    # Common parameters\n",
    "    N = 10  # number of top probes to report\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 1: Select Probes with the Highest Variance\n",
    "    # =============================================================================\n",
    "    probe_variances = data_matrix.var(axis=0)\n",
    "    top_variance_probes = probe_variances.nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_variance_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'var_1',\n",
    "            'probe': probe,\n",
    "            'variance': probe_variances[probe],\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 2: Select Probes with the Highest Correlation to SITH Score\n",
    "    # =============================================================================\n",
    "    correlations = data_matrix.apply(lambda x: x.corr(sorted_df['SITH']), axis=0)\n",
    "    top_correlated_probes = correlations.abs().nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_correlated_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'corr_2',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': correlations[probe],\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 3: Perform Statistical Tests (ANOVA) Between Groups\n",
    "    # =============================================================================\n",
    "    data_matrix_with_labels = data_matrix.copy()\n",
    "    data_matrix_with_labels.reset_index(drop=True, inplace=True)\n",
    "    sorted_df.reset_index(drop=True, inplace=True)\n",
    "    data_matrix_with_labels['sith_level'] = sorted_df['sith_level']\n",
    "    subset_cols = ['sith_level'] + probe_cols\n",
    "    data_matrix_with_labels.dropna(subset=subset_cols, inplace=True)\n",
    "\n",
    "    p_values = {}\n",
    "    for probe in probe_cols:\n",
    "        groups = []\n",
    "        for level in data_matrix_with_labels['sith_level'].unique():\n",
    "            group = data_matrix_with_labels[data_matrix_with_labels['sith_level'] == level][probe]\n",
    "            groups.append(group)\n",
    "        if len(groups) >= 2:\n",
    "            stat, p_value = f_oneway(*groups)\n",
    "            p_values[probe] = p_value\n",
    "\n",
    "    p_values_series = pd.Series(p_values).dropna()\n",
    "    top_pvalue_probes = p_values_series.nsmallest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_pvalue_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'anova_3',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': p_values[probe],\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 4a: Feature Selection Using Random Forest (with Imputation)\n",
    "    # =============================================================================\n",
    "    X = data_matrix.copy()\n",
    "    y = sorted_df['SITH']\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    rf_impute = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_impute.fit(X_imputed, y)\n",
    "    importances_impute = pd.Series(rf_impute.feature_importances_, index=X_imputed.columns)\n",
    "    top_importance_probes_impute = importances_impute.nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_importance_probes_impute, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'rf_impute_4a',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': importances_impute[probe],\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 4b: Feature Selection Using Random Forest (dropping rows with NaNs)\n",
    "    # =============================================================================\n",
    "    X = data_matrix.copy()\n",
    "    y = sorted_df['SITH']\n",
    "    data_combined = X.copy()\n",
    "    data_combined['SITH'] = y\n",
    "    data_combined.dropna(inplace=True)\n",
    "    X_dropped = data_combined.drop(columns=['SITH'])\n",
    "    y_dropped = data_combined['SITH']\n",
    "\n",
    "    rf_drop = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_drop.fit(X_dropped, y_dropped)\n",
    "    importances_drop = pd.Series(rf_drop.feature_importances_, index=X_dropped.columns)\n",
    "    top_importance_probes_drop = importances_drop.nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_importance_probes_drop, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'rf_drop_4b',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': importances_drop[probe],\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 5: Common Probes of Interest\n",
    "    # =============================================================================\n",
    "    probes_of_interest = [\n",
    "        'cg00870279','cg01842321','cg01861555','cg06805320','cg07235253','cg08327371',\n",
    "        'cg08532569','cg08598483','cg10982913','cg12164232','cg14387626','cg16272777',\n",
    "        'cg16338877','cg16401270','cg17163967','cg19477190','cg21142743','cg22635155',\n",
    "        'cg27086014'\n",
    "    ]\n",
    "    # Keep only those that passed the filtering\n",
    "    probes_of_interest = [p for p in probes_of_interest if p in probe_cols]\n",
    "    for rank, probe in enumerate(probes_of_interest, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'common_5',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 6: Combine Multiple Criteria (variance + ANOVA p-value)\n",
    "    # =============================================================================\n",
    "    top_50_variance_probes = probe_variances.nlargest(50).index.tolist()\n",
    "    p_values_combined = {}\n",
    "    for probe in top_50_variance_probes:\n",
    "        groups = []\n",
    "        for level in sorted_df['sith_level'].unique():\n",
    "            group = sorted_df[sorted_df['sith_level'] == level][probe]\n",
    "            groups.append(group)\n",
    "        if len(groups) >= 2:\n",
    "            stat, p_val = f_oneway(*groups)\n",
    "            p_values_combined[probe] = p_val\n",
    "\n",
    "    top_combined_probes = pd.Series(p_values_combined).nsmallest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_combined_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'combined_6',\n",
    "            'probe': probe,\n",
    "            'variance': probe_variances[probe],\n",
    "            'correlation': np.nan,\n",
    "            'p_value': p_values_combined[probe],\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 7: Clustering and Selecting Representative Probes\n",
    "    # =============================================================================\n",
    "    X_transposed = data_matrix.T.dropna(how='all')  # probes as rows\n",
    "    X_transposed_imputed = X_transposed.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    X_transposed_imputed = X_transposed_imputed.dropna(how='any', axis=0)\n",
    "    n_clusters = 10\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = clustering.fit_predict(X_transposed_imputed)\n",
    "    clustered_probes = pd.DataFrame({'probe': X_transposed_imputed.index, 'cluster': cluster_labels})\n",
    "    selected_probes = clustered_probes.groupby('cluster')['probe'].first().tolist()\n",
    "    \n",
    "    # For each cluster's \"representative\" probe\n",
    "    for clust, probe in zip(sorted(clustered_probes['cluster'].unique()), selected_probes):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'cluster_7',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': clust\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 8: Filtering Based on Methylation Range\n",
    "    # =============================================================================\n",
    "    probe_ranges = data_matrix.max(axis=0) - data_matrix.min(axis=0)\n",
    "    top_range_probes = probe_ranges.nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_range_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'range_8',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': probe_ranges[probe],\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 9: Filtering Based on Specific Thresholds\n",
    "    # =============================================================================\n",
    "    high_threshold = 0.8\n",
    "    low_threshold = 0.2\n",
    "    high_proportion = (data_matrix > high_threshold).mean(axis=0)\n",
    "    low_proportion = (data_matrix < low_threshold).mean(axis=0)\n",
    "    selected_probes_threshold = high_proportion[high_proportion > 0.3].index.intersection(\n",
    "                                low_proportion[low_proportion > 0.3].index).tolist()\n",
    "    selected_probes_threshold = selected_probes_threshold[:N]  # limit to top N\n",
    "    for rank, probe in enumerate(selected_probes_threshold, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'threshold_9',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': high_proportion[probe],\n",
    "            'prop_below_0.2': low_proportion[probe],\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 10: Differential Methylation Analysis Using Linear Regression\n",
    "    # =============================================================================\n",
    "    data_for_analysis = data_matrix.copy()\n",
    "    data_for_analysis.reset_index(drop=True, inplace=True)\n",
    "    sorted_df.reset_index(drop=True, inplace=True)\n",
    "    data_for_analysis['sith_level'] = sorted_df['sith_level']\n",
    "    subset_cols = ['sith_level'] + probe_cols\n",
    "    data_for_analysis.dropna(subset=subset_cols, inplace=True)\n",
    "    # Convert categorical level to numeric codes\n",
    "    data_for_analysis['sith_level_code'] = data_for_analysis['sith_level'].cat.codes\n",
    "    \n",
    "    p_values_differential = {}\n",
    "    for probe in probe_cols:\n",
    "        formula = f\"Q('{probe}') ~ sith_level_code\"\n",
    "        try:\n",
    "            model = smf.ols(formula, data=data_for_analysis).fit()\n",
    "            p_values_differential[probe] = model.pvalues['sith_level_code']\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    p_values_differential = {k: v for k, v in p_values_differential.items() if not pd.isnull(v)}\n",
    "    top_differential_probes = pd.Series(p_values_differential).nsmallest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_differential_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'diffmeth_10',\n",
    "            'probe': probe,\n",
    "            'variance': np.nan,\n",
    "            'correlation': np.nan,\n",
    "            'p_value': p_values_differential[probe],\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': np.nan,\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': np.nan,\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "    # =============================================================================\n",
    "    # Method 11: Visualizing All Filters with an Aggregate Score\n",
    "    # =============================================================================\n",
    "    criteria_df = pd.DataFrame(index=probe_cols)\n",
    "    criteria_df['variance'] = probe_variances\n",
    "    criteria_df['correlation'] = correlations.abs()\n",
    "    criteria_df['range'] = probe_ranges\n",
    "    # Insert ANOVA p_values, fill missing with 1.0 so they don't artificially rank well\n",
    "    criteria_df['p_value'] = p_values_series.reindex(probe_cols).fillna(1.0)\n",
    "\n",
    "    # Normalize columns for comparability\n",
    "    scaler = MinMaxScaler()\n",
    "    subset_for_scaling = criteria_df[['variance', 'correlation', 'range', 'p_value']].fillna(0)\n",
    "    criteria_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(subset_for_scaling),\n",
    "        index=subset_for_scaling.index,\n",
    "        columns=['variance', 'correlation', 'range', 'p_value']\n",
    "    )\n",
    "    # Invert the p_value scale so that lower p-values score higher\n",
    "    criteria_normalized['p_value'] = 1 - criteria_normalized['p_value']\n",
    "    criteria_normalized['aggregate_score'] = criteria_normalized.mean(axis=1)\n",
    "\n",
    "    top_aggregate_probes = criteria_normalized['aggregate_score'].nlargest(N).index.tolist()\n",
    "    for rank, probe in enumerate(top_aggregate_probes, start=1):\n",
    "        results_list.append({\n",
    "            'dataset': name,\n",
    "            'method': 'aggregate_11',\n",
    "            'probe': probe,\n",
    "            'variance': criteria_df.loc[probe, 'variance'],\n",
    "            'correlation': criteria_df.loc[probe, 'correlation'],\n",
    "            'p_value': criteria_df.loc[probe, 'p_value'],\n",
    "            'rf_importance_impute': np.nan,\n",
    "            'rf_importance_drop': np.nan,\n",
    "            'range': criteria_df.loc[probe, 'range'],\n",
    "            'prop_above_0.8': np.nan,\n",
    "            'prop_below_0.2': np.nan,\n",
    "            'aggregate_score': criteria_normalized.loc[probe, 'aggregate_score'],\n",
    "            'cluster_label': np.nan\n",
    "        })\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Create a final results DataFrame from our accumulated rows and save to CSV\n",
    "# -------------------------------------------------------------------------\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv('../_OUTPUTS_/pcawg_probes_of_interest.csv', index=False)\n",
    "\n",
    "print(\"Done! Results saved to '../_OUTPUTS_/pcawg_probes_of_interest.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>method</th>\n",
       "      <th>probe</th>\n",
       "      <th>variance</th>\n",
       "      <th>correlation</th>\n",
       "      <th>p_value</th>\n",
       "      <th>rf_importance_impute</th>\n",
       "      <th>rf_importance_drop</th>\n",
       "      <th>range</th>\n",
       "      <th>prop_above_0.8</th>\n",
       "      <th>prop_below_0.2</th>\n",
       "      <th>aggregate_score</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pcawg_prim_window</td>\n",
       "      <td>var_1</td>\n",
       "      <td>cg00840341</td>\n",
       "      <td>0.047657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pcawg_prim_window</td>\n",
       "      <td>var_1</td>\n",
       "      <td>cg13363969</td>\n",
       "      <td>0.043906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pcawg_prim_window</td>\n",
       "      <td>var_1</td>\n",
       "      <td>cg15992272</td>\n",
       "      <td>0.043533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pcawg_prim_window</td>\n",
       "      <td>var_1</td>\n",
       "      <td>cg15618210</td>\n",
       "      <td>0.043486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pcawg_prim_window</td>\n",
       "      <td>var_1</td>\n",
       "      <td>cg08566882</td>\n",
       "      <td>0.043257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>pcawg_iqr_corr</td>\n",
       "      <td>aggregate_11</td>\n",
       "      <td>cg14166284</td>\n",
       "      <td>0.030293</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.777561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733290</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>pcawg_iqr_corr</td>\n",
       "      <td>aggregate_11</td>\n",
       "      <td>cg19295951</td>\n",
       "      <td>0.046240</td>\n",
       "      <td>0.330906</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.732978</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>pcawg_iqr_corr</td>\n",
       "      <td>aggregate_11</td>\n",
       "      <td>cg23882019</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>0.444167</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.723090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>pcawg_iqr_corr</td>\n",
       "      <td>aggregate_11</td>\n",
       "      <td>cg08328777</td>\n",
       "      <td>0.044698</td>\n",
       "      <td>0.307794</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.880604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722356</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>pcawg_iqr_corr</td>\n",
       "      <td>aggregate_11</td>\n",
       "      <td>cg17850597</td>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.417762</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset        method       probe  variance  correlation  \\\n",
       "0    pcawg_prim_window         var_1  cg00840341  0.047657          NaN   \n",
       "1    pcawg_prim_window         var_1  cg13363969  0.043906          NaN   \n",
       "2    pcawg_prim_window         var_1  cg15992272  0.043533          NaN   \n",
       "3    pcawg_prim_window         var_1  cg15618210  0.043486          NaN   \n",
       "4    pcawg_prim_window         var_1  cg08566882  0.043257          NaN   \n",
       "..                 ...           ...         ...       ...          ...   \n",
       "327     pcawg_iqr_corr  aggregate_11  cg14166284  0.030293     0.489655   \n",
       "328     pcawg_iqr_corr  aggregate_11  cg19295951  0.046240     0.330906   \n",
       "329     pcawg_iqr_corr  aggregate_11  cg23882019  0.028151     0.444167   \n",
       "330     pcawg_iqr_corr  aggregate_11  cg08328777  0.044698     0.307794   \n",
       "331     pcawg_iqr_corr  aggregate_11  cg17850597  0.038790     0.417762   \n",
       "\n",
       "      p_value  rf_importance_impute  rf_importance_drop     range  \\\n",
       "0         NaN                   NaN                 NaN       NaN   \n",
       "1         NaN                   NaN                 NaN       NaN   \n",
       "2         NaN                   NaN                 NaN       NaN   \n",
       "3         NaN                   NaN                 NaN       NaN   \n",
       "4         NaN                   NaN                 NaN       NaN   \n",
       "..        ...                   ...                 ...       ...   \n",
       "327  0.001336                   NaN                 NaN  0.777561   \n",
       "328  0.000072                   NaN                 NaN  0.867622   \n",
       "329  0.000005                   NaN                 NaN  0.823488   \n",
       "330  0.000038                   NaN                 NaN  0.880604   \n",
       "331  0.000726                   NaN                 NaN  0.776620   \n",
       "\n",
       "     prop_above_0.8  prop_below_0.2  aggregate_score  cluster_label  \n",
       "0               NaN             NaN              NaN            NaN  \n",
       "1               NaN             NaN              NaN            NaN  \n",
       "2               NaN             NaN              NaN            NaN  \n",
       "3               NaN             NaN              NaN            NaN  \n",
       "4               NaN             NaN              NaN            NaN  \n",
       "..              ...             ...              ...            ...  \n",
       "327             NaN             NaN         0.733290            NaN  \n",
       "328             NaN             NaN         0.732978            NaN  \n",
       "329             NaN             NaN         0.723090            NaN  \n",
       "330             NaN             NaN         0.722356            NaN  \n",
       "331             NaN             NaN         0.720308            NaN  \n",
       "\n",
       "[332 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
